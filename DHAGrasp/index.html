<!DOCTYPE html>
<html lang="en">

<head>
  <meta name="google-site-verification" content="DOiC7afK4KluEKATjy_DTThhZgLh6My7TjPNXdbI8u4" />
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions</title>
  <!-- Bootstrap -->
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
  <style>
    body {
      background: rgb(255, 255, 255) no-repeat fixed top left;
      font-family: 'Open Sans', sans-serif;
    }
  </style>

</head>

<!-- cover -->
<section>
  <div class="jumbotron text-center mt-0">
    <div class="container-fluid">
      <div class="row">
        <div class="col">
          <h2 style="font-size:30px;">DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions</h2>
          <h4 style="color:#6e6e6e;"></h4>
          <hr>
          <h6>
            <a href="https://quanzhou-li.github.io/">Quanzhou Li</a><sup>1</sup>&nbsp; &nbsp;
            <a href="https://wu-zhonghua.github.io/">Zhonghua Wu</a><sup>2</sup>&nbsp; &nbsp;
            <a href="https://wangjingbo1219.github.io/">Jingbo Wang</a><sup>3</sup>&nbsp; &nbsp;
            <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a><sup>1,†</sup>&nbsp; &nbsp;
            <a href="https://daibo.info/">Bo Dai</a><sup>4</sup>
            <br>
            <br>
            <p> <sup>1</sup>S-Lab, Nanyang Technological University&nbsp; &nbsp;
              <sup>2</sup>SenseTime Research; &nbsp;
              <sup>3</sup>Shanghai AI Laboratory; &nbsp;
              <sup>4</sup>The University of Hong Kong&nbsp; &nbsp;
              <br>
            </p>
            <p>
              <sup>†</sup> corresponding author &nbsp;
              <br>
            </p>

            <div class="row justify-content-center">
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://quanzhou-li.github.io/DHAGrasp/" role="button"
                    target="_blank">
                    <i class="fa fa-file"></i> arxiv </a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon"
                    href="https://quanzhou-li.github.io/DHAGrasp/" role="button" target="_blank" disabled=1>
                    <i class="fa fa-github-alt"></i> Code (coming soon) </a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon"
                    href="https://quanzhou-li.github.io/DHAGrasp/" role="button" target="_blank"
                    disabled=1>
                    <i class="fa fa-github-alt"></i> Dataset (coming soon) </a> </p>
              </div>
            </div>
            

        </div>
      </div>
    </div>
  </div>
</section>


<!-- abstract -->
<section>
  <div class="container" style="width:58%">
    <div class="row">
      <div class="col-12">
        <h2><strong>Abstract</strong></h2>
        <hr style="margin-top:0px">
        <p class="text-justify">
          Learning to generate dual-hand grasps that respect object semantics is essential for robust hand–object interaction but remains largely underexplored due to dataset scarcity. 
          Existing grasp datasets predominantly focus on single-hand interactions and contain only limited semantic part annotations. 
          To address these challenges, we introduce a pipeline, SymOpt, that constructs a large-scale dual-hand grasp dataset by leveraging existing single-hand datasets and exploiting object and hand symmetries. 
          Building on this, we propose a text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand Affordance-aware Grasps for unseen objects. 
          Our approach incorporates a novel dual-hand affordance representation and follows a two-stage design, 
          which enables effective learning from a small set of segmented training objects while scaling to a much larger pool of unsegmented data. 
          Extensive experiments demonstrate that our method produces diverse and semantically consistent grasps, outperforming strong baselines in both grasp quality and generalization to unseen objects.
        </p>
      </div>
    </div>
  </div>
</section>
<br>

<section>
  <div class="container" style="width:58%">
    <div class="row">
      <div class="col-12">
        <h2><strong>Video</strong></h2>
        <hr style="margin-top:0px">
        <div class="row justify-content-center" style="align-items:center; display:flex;">
          <video width="90%" playsinline="" preload="" muted="" controls>
            <source src="videos/Supplementary_video.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<br>
<br>

<section>
  <div class="container" style="width:58%">
    <div class="row">
      <div class="col-12">
        <!-- <h2><strong>Qualitative results</strong></h2> -->
        <h2><strong>set</strong></h2>
        <hr style="margin-top:0px">
        <img src="images/set.pdf" width="100%" style="margin-bottom:20px;">
        <p class="text-justify">
          <!-- Some diverse grasps on the objects from DexGraspNet. -->
          Some synthetic training scenes and grasping pose from DexGraspNet 2.0.
        </p>
      </div>
    </div>
  </div>
  </div>
</section>
<br>
<br>
<section></section>
  <div class="container" style="width:58%">
    <div class="row">
      <div class="col-12">
        <h2><strong>Qualitative results</strong></h2> 
        <!-- <h2><strong>set</strong></h2> -->
        <hr style="margin-top:0px">
        <img src="images/result_1.png" width="45%" style="margin-bottom:20px;">
        <img src="images/result_2.png" width="45%" style="margin-bottom:20px;">
        <img src="images/result_3.png" width="45%" style="margin-bottom:20px;">
        <img src="images/result_4.png" width="45%" style="margin-bottom:20px;">
        <img src="images/result_5.png" width="45%" style="margin-bottom:20px;">
        <img src="images/result_6.png" width="45%" style="margin-bottom:20px;">
        <img src="images/result_7.png" width="45%" style="margin-bottom:20px;">
        <img src="images/result_8.png" width="45%" style="margin-bottom:20px;">
        
        <p class="text-justify">
          <!-- Some diverse grasps on the objects from DexGraspNet. -->
          Dexterous grasping generation and real-world execution in a cluttered scene.
        </p>
      </div>
    </div>
  </div>
  </div>
</section>


<br>
<br>

<!-- citing -->
<div class="container" style="width:58%">
  <div class="row ">
    <div class="col-12">
      <h2><strong>Citation</strong></h2>
      <hr style="margin-top:0px">
      <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>
  
</code></pre>
    </div>
  </div>
</div>
<br>



<footer class="text-center" style="margin-bottom:10px; font-size: medium;">
  <hr>
  Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a
    href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
</footer>
<script>
  MathJax = {
    tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>

</html>

