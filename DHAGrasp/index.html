<!DOCTYPE html>
<html lang="en">

<head>
  <meta name="google-site-verification" content="DOiC7afK4KluEKATjy_DTThhZgLh6My7TjPNXdbI8u4" />
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions</title>
  <!-- Bootstrap -->
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
  <style>
    body {
      background: rgb(255, 255, 255) no-repeat fixed top left;
      font-family: 'Open Sans', sans-serif;
    }
  </style>

</head>

<!-- cover -->
<section>
  <div class="jumbotron text-center mt-0">
    <div class="container-fluid">
      <div class="row">
        <div class="col">
          <h2 style="font-size:30px;">DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions</h2>
          <h4 style="color:#ffffff;"></h4>
          <hr>
          <h6>
            <a href="https://quanzhou-li.github.io/">Quanzhou Li</a><sup>1</sup>&nbsp; &nbsp;
            <a href="https://wu-zhonghua.github.io/">Zhonghua Wu</a><sup>2</sup>&nbsp; &nbsp;
            <a href="https://wangjingbo1219.github.io/">Jingbo Wang</a><sup>3</sup>&nbsp; &nbsp;
            <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a><sup>1,†</sup>&nbsp; &nbsp;
            <a href="https://daibo.info/">Bo Dai</a><sup>4</sup>
            <br>
            <br>
            <p> <sup>1</sup>S-Lab, Nanyang Technological University&nbsp; &nbsp;
              <sup>2</sup>SenseTime Research; &nbsp;
              <sup>3</sup>Shanghai AI Laboratory; &nbsp;
              <sup>4</sup>The University of Hong Kong&nbsp; &nbsp;
              <br>
            </p>
            <p>
              <sup>†</sup> corresponding author &nbsp;
              <br>
            </p>

            <div class="row justify-content-center">
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://quanzhou-li.github.io/DHAGrasp/" role="button"
                    target="_blank">
                    <i class="fa fa-file"></i> arxiv </a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon"
                    href="https://quanzhou-li.github.io/DHAGrasp/" role="button" target="_blank" disabled=1>
                    <i class="fa fa-github-alt"></i> Code (coming soon) </a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon"
                    href="https://quanzhou-li.github.io/DHAGrasp/" role="button" target="_blank"
                    disabled=1>
                    <i class="fa fa-github-alt"></i> Dataset (coming soon) </a> </p>
              </div>
            </div>
            

        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div class="container" style="width:58%">
    <div class="row">
      <div class="col-12">
        <hr style="margin-top:0px">
        <img src="images/teaser.jpg" width="100%" style="margin-bottom:20px;">
      </div>
    </div>
  </div>
  </div>
</section>
<br>
<br>


<!-- abstract -->
<section>
  <div class="container" style="width:58%">
    <div class="row">
      <div class="col-12">
        <h2><strong>Abstract</strong></h2>
        <hr style="margin-top:0px">
        <p class="text-justify">
          Learning to generate dual-hand grasps that respect object semantics is essential for robust hand–object interaction but remains largely underexplored due to dataset scarcity. 
          Existing grasp datasets predominantly focus on single-hand interactions and contain only limited semantic part annotations. 
          To address these challenges, we introduce a pipeline, SymOpt, that constructs a large-scale dual-hand grasp dataset by leveraging existing single-hand datasets and exploiting object and hand symmetries. 
          Building on this, we propose a text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand Affordance-aware Grasps for unseen objects. 
          Our approach incorporates a novel dual-hand affordance representation and follows a two-stage design, 
          which enables effective learning from a small set of segmented training objects while scaling to a much larger pool of unsegmented data. 
          Extensive experiments demonstrate that our method produces diverse and semantically consistent grasps, outperforming strong baselines in both grasp quality and generalization to unseen objects.
        </p>
      </div>
    </div>
  </div>
</section>
<br>
  
<section></section>
  <div class="container" style="width:58%">
    <div class="row">
      <div class="col-12">
        <h2><strong>Method</strong></h2> 
        <!-- <h2><strong>set</strong></h2> -->
        <hr style="margin-top:0px">
        <img src="images/pipeline.jpg" width="100%" style="margin-bottom:20px;">
        <p class="text-justify">
          Our work consists of two main phases: data generation and grasp synthesis. 
          In the data generation phase, we first mirror right-hand grasps across the pseudo-symmetry plane of the object to obtain left-hand grasp proposals. 
          By combining the right and left grasps, we then apply an energy-based optimization scheme to construct two-hand grasps. 
          In the grasp synthesis phase, we propose a two-hand contact representation, as illustrated in the second box, and a two-stage grasp generation pipeline, outlined in the third box. 
          In the first stage of the generator, we design a diffusion model Text2Dir, conditioned on the object and text, to predict the affordance directions of a grasp. 
          The object and the predicted affordance directions are subsequently passed to our Dir2Grasp model to generate the final grasp.
        </p>
      </div>
    </div>
  </div>
  </div>
</section>
<br>
<br>

<section>
  <div class="container" style="width:58%">
    <div class="row">
      <div class="col-12">
        <h2><strong>Video</strong></h2>
        <hr style="margin-top:0px">
        <div class="row justify-content-center" style="align-items:center; display:flex;">
          <video width="90%" playsinline="" preload="" muted="" controls>
            <source src="videos/Supplementary_video.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<br>
<br>
  
<section></section>
  <div class="container" style="width:58%">
    <div class="row">
      <div class="col-12">
        <h2><strong>Qualitative results</strong></h2> 
        <!-- <h2><strong>set</strong></h2> -->
        <hr style="margin-top:0px">
        <img src="images/qual_1.jpg" width="100%" style="margin-bottom:20px;">
        <img src="images/qual_2.jpg" width="100%" style="margin-bottom:20px;">
        
        <p class="text-justify">
          Generated grasps on unseen objects with DHAGrasp.
        </p>
      </div>
    </div>
  </div>
  </div>
</section>


<br>
<br>

<!-- citing -->
<div class="container" style="width:58%">
  <div class="row ">
    <div class="col-12">
      <h2><strong>Citation</strong></h2>
      <hr style="margin-top:0px">
      <pre style="background-color: #ffffff;padding: 1.25em 1.5em">
<code>
  
</code></pre>
    </div>
  </div>
</div>
<br>



<footer class="text-center" style="margin-bottom:10px; font-size: medium;">
  <hr>
  Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a
    href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
</footer>
<script>
  MathJax = {
    tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>

</html>

