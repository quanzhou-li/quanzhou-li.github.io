<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!--<link rel="icon" href="resources/manipulator.ico"> -->
  <title>Quanzhou Li</title>
  <link href='https://fonts.googleapis.com/css?family=Ovo' rel='stylesheet' type='text/css'>
  <style type="text/css">
    body {
      font-family: 'Ovo', serif;
    }
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    .compact {
      margin-block-start: 0.2em;
      margin-block-end: 0.2em;
    }

    #contact{
      font-size: 9pt;
      font-family: "Courier New";
    }

    #photo {
      float: right;
    }

    #photo img {
      width: 200px;
      border-radius: 20px;
      margin: 0 0 1em 1em;
    }

    @media (max-width: 600px) {
      #photo img {
        width: 140px;
        border-radius: 50%;
      }
      .researchimage{
        width: 100%;
        border-radius: 5px;
      }
    }

    #container {
      margin: 0 5px 0 5px;
    }

    @media (min-width: 600px) {
      #container {
        width: 700px;
        margin: 0 auto;
        margin-top: 60px;
      }
      .researchimagetd{
         width: 30%;
      }
      .researchtext{
        width: 70%;
      }
      .researchimage{
        width: 100%;
        border-radius: 5px;
      }
    }

    @media screen and (max-width: 600px) {
      table tr {
        display: block;
      }
  
      table td {
        display: block;
      }

      .researchimagetd{
        width: 100%;
        text-align: center;
        margin-top: 1em;
      }
      .researchtext{
        width: 95%;
        text-align: left;
      }
      .researchimage{
        width: 50%;
        margin-right: 2em;

      }
    }
  </style>

</head>

<body>

  <div id="container">
    <div id="photo">
      <img src="Screenshot_20201119-131830_Gallery.jpg" alt="Photo">
    </div>

    <h1>Quanzhou Li</h1>
    <p id="contact">
      Email: quanzhou001[at]e.ntu.edu.sg
    </p>

    <p>
      Welcome to my website!
      <br><br>
      I'm Quanzhou Li and am currently a PhD student at Nanyang Technological University, supervised by Prof. <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a> 
      and Prof. <a href="http://daibo.info/">Bo Dai</a>. I earned my Bachelor's degree from the University of Toronto with <i>High Distinction</i> in 2020. I also spent two years 
      at Beihang University, China, before transferring to U of T in 2018. 
      <br><be>
      <!-- My research interests mainly lie in digital human and 3D computer vision. -->
    </p>

    <h3>Research Interests</h3>
    <ul>
      <li>Digital human and generative models.</li>
      <li>Reinforcement learning and minipulations.</li>
    </ul>
    
    <br><br>

    <h3 style="margin-block-end: 0em">Research Projects</h3>
    <table class="compact" width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      
      <tbody style="margin-block-start: 0em"><tr>
        <td class="researchimagetd">
          <div>
              <img src="TOHO.jpg" class="researchimage">
          </div>
        </td>
        <td valign="top" class="researchtext">
          <p class="compact"><a href="https://arxiv.org/abs/2303.13129"><i>	Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations</i></a></p>
          <p class="compact"><b>Quanzhou Li</b>, Jingbo Wang, Chen Change Loy, Bo Dai</p>
          <b></b><em>WACV 2024</em></b>
          <p class="compact"> 
            <em>In this paper, we propose a method that generates full human-object interaction motions to conduct specific tasks. Our generated motions are continuous and allow upsampling to arbitrary frames.</em>
          </p>
          <a href="https://arxiv.org/abs/2303.13129">paper</a>
        </td>
        </tr>
      </tbody>
      
      <tbody style="margin-block-start: 0em"><tr>
        <td class="researchimagetd">
          <div>
              <img src="DHAGrasp/images/teaser.jpg" class="researchimage">
          </div>
        </td>
        <td valign="top" class="researchtext">
          <p class="compact"><a href=""><i>	DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions</i></a></p>
          <p class="compact"><b>Quanzhou Li</b>, Zhonghua Wu, Jingbo Wang, Chen Change Loy, Bo Dai</p>
          <b></b><em>arxiv</em></b>
          <p class="compact"> 
            <em>In this paper, we propose SymOpt for large-scale dual-hand grasp data generation and DHAGrasp, a text-guided two-stage model that synthesizes diverse dual-hand affordance-aware grasps for unseen objects.</em>
          </p>
          <a href="">paper</a>
        </td>
        </tr>
      </tbody>
      
      <tbody style="margin-block-start: 0em"><tr>
        <td class="researchimagetd">
          <div>
              <img src="LbW.gif" class="researchimage">
          </div>
        </td>
        <td valign="top" class="researchtext">
          <p class="compact"><a href="https://www.pair.toronto.edu/lbw-kp/"><i>	Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</i></a></p>
          <p class="compact">Haoyu Xiong, <b>Quanzhou Li</b>, Yun-Chun Chen, Homanga Bharadhwaj, Samrath Sinha, Animesh Garg</p>
          <b></b><em>IROS 2021</em></b>
          <p class="compact"> 
            <em>In this paper we develop a perception module that learns to translate human videos to the robot domain followed by keypoint detection learned in an unsupervised fashion. The key insight of our method lies in explicitly exploiting the kinematics and motion information embedded in the video to learn structured representations.</em>
          </p>
          <a href="https://www.pair.toronto.edu/lbw-kp/">website</a> / <a href="https://youtu.be/Retu1q-BbEo">video</a> / <a href="https://arxiv.org/pdf/2101.07241.pdf">paper</a>
        </td>
        </tr>
      </tbody>
    </table>
  </div>
</body></html>
